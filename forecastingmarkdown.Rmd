---
title: "Forecast Markdown"
output: html_document
date: "2023-06-13"
---
#Libraries
```{r setup, include=FALSE}
# 1.0 LIBRARIES ----
# Machine Learning
library(tidymodels)
library(modeltime)
library(modeltime.ensemble)
library(modeltime.resample)

# Time Series
library(timetk)

# Core
library(tidyverse)
library(trelliscopejs)
library(janitor)

#EDA
library(DataExplorer)


# Machine Learning
library(tidymodels)
library(modeltime)
library(modeltime.ensemble)
library(modeltime.resample)
library(prophet)
library(rules)
library(trelliscopejs)
library(ranger)
library(randomForest)
library(recipes)
library(kknn)
library(kernlab)

library(thief)
library(Cubist)

options(scipen = 9999)
```





```{r}
walmart_sales_weekly

walmart_sales_weekly %>%
    group_by(id) %>%
    plot_time_series(
        Date, Weekly_Sales,
        .facet_ncol = 3, 
        .interactive = TRUE,
        .trelliscope = TRUE)
```




# 1.0 Data Prep

```{r}
sales_prepared_tbl <- walmart_sales_weekly %>% 
  clean_names() %>% 
  select(id, date, weekly_sales, is_holiday, temperature, fuel_price, cpi, unemployment)
```




## EDA
```{r}
DataExplorer::plot_missing(sales_prepared_tbl)
```

#2.0 Time Series Prep
## Anomaly Detection
```{r}
anomaly_detection_tbl <- sales_prepared_tbl %>%
    
    # global changes
    mutate(weekly_sales = log1p(weekly_sales)) %>%
    group_by(id) %>%
    
    # anomaly detection
    # plot_anomaly_diagnostics(.date_var = date, .value = weekly_sales, .facet_ncol = 2) %>%
    tk_anomaly_diagnostics(.date_var = date, .value = weekly_sales) %>%
    mutate(anomaly = ifelse(anomaly == "Yes", 1, 0)) %>%
    select(id, date, anomaly) %>%
    ungroup()
  
  
  
  
  plot_time_series(.date_var = date,
                   .value = weekly_sales,
                   .facet_ncol = 3,
                   .facet_nrow = 2,
                   .smooth = F,
                   .trelliscope = T)
```


## ACF Diagnostics

```{r}
sales_prepared_tbl %>% 
  group_by(id) %>% 
  plot_acf_diagnostics(.date_var = date, .value = weekly_sales)
```


#Full Dataset Feature Engineering (external regressors)
```{r}
full_data_prepared_tbl <- sales_prepared_tbl %>%
    
    # global changes
    mutate(weekly_sales = log1p(weekly_sales)) %>%
    group_by(id) %>%
    
    # pad any missing dates
    pad_by_time(
        .date_var = date, 
        .by = "week", 
        .pad_value = NA
        # .end_date = "2012-12-31"
    ) %>%
    ungroup() %>%
    
    # impute missing sales data
    mutate(weekly_sales = ts_impute_vec(weekly_sales, period = 4)) %>%
    

    # create future frame
    group_by(id) %>%
    future_frame(
        .date_var = date, 
        .length_out = 24, 
        .bind_data = TRUE) %>%
    ungroup() %>%
    
    # change characters to factors
    mutate(id = as.factor(id)) %>%
    
    # fourier, lags & slidify
    group_by(id) %>%
    tk_augment_fourier(
        .date_var = date, 
        .periods  = c(28, 52)
    ) %>%
    tk_augment_lags(
        .value = weekly_sales, 
        .lags = 24
    ) %>% 
    tk_augment_slidify(
        str_glue("weekly_sales_lag24"),
        .f = ~mean(.x, na.rm = TRUE),
        .period = 24,
        .partial = TRUE,
        .align = "center"
    ) %>%
    ungroup() %>%
    
    # add xregs
    group_by(id) %>%
    left_join(anomaly_detection_tbl, by = c("id", "date")) %>%
    mutate(anomaly = ifelse(is.na(anomaly), 0, anomaly)) %>%
    ungroup() %>%
    
    # impute missing xreg data
    group_by(id) %>%
    mutate(is_holiday = as.numeric(is_holiday)) %>%
    mutate(across(.cols = c(is_holiday:unemployment),
                  .fns  = ~ifelse(is.na(.x), ts_impute_vec(.x, period = 52), .x))) %>%
  mutate(is_holiday = round(is_holiday)) %>%
    ungroup() %>%
    
    # drop na's from lag
    drop_na(str_glue("weekly_sales_lag24"))
    


full_data_prepared_tbl %>%
    group_by(id) %>%
    plot_time_series(
        .date_var = date, 
        .value = weekly_sales, 
        .facet_ncol = 3,
        .facet_nrow = 2,
        .smooth = FALSE, 
        .trelliscope = FALSE)
```



```{r}
#prepared dataset
weekly_prepared_tbl <- full_data_prepared_tbl %>% 
  filter(!is.na(weekly_sales)) %>% 
  drop_na()
#future dataset
future_prepared_tbl <- full_data_prepared_tbl %>% 
  group_by(id) %>% 
  filter(is.na(weekly_sales))
```




```{r}
future_prepared_tbl <- future_prepared_tbl %>%
  group_by(id) %>%
  mutate(across(.cols = contains("lag"), 
                .fns  = ~ ifelse(is.nan(.x), NA, .x))) %>%
  mutate(across(.cols = contains("lag"),
                .fns  = ~ replace_na(.x, 0)))


# check for NA's (should be zero)
future_prepared_tbl %>% filter(is.na(str_glue("weekly_sales_lag24_roll_24")))
```







## Deal with NA/NaN's in future data
```{r}
future_prepared_tbl <- future_prepared_tbl %>%
  group_by(id) %>%
  mutate(across(.cols = contains("lag"),
                .fns  = ~ ifelse(is.nan(.x), NA, .x))) %>%
  mutate(across(.cols = contains("lag"),
                .fns  = ~ replace_na(.x, 0)))
# check for NA's (should be zero)
future_prepared_tbl %>% filter(is.na(str_glue("weekly_sales_lag24_roll_24")))
```




#3.0 Train/Test Data

```{r}
splits <- weekly_prepared_tbl %>% 
  time_series_split(date_var = date, assess = 24, cumulative = TRUE)

splits %>% 
  tk_time_series_cv_plan() %>% 
  plot_time_series_cv_plan(.date_var=date, .value = weekly_sales)

splits %>% 
  tk_time_series_cv_plan() %>% 
  filter(.key == "testing")
```

#4.0 RECIPE
```{r}


training(splits)
testing(splits)


#sequential recipie
recipe_spec<-recipe(weekly_sales ~ ., training(splits)) %>% 
  #clean up factor leveling
  step_mutate_at(id, fn = droplevels) %>% 
  step_normalize(temperature:unemployment) %>% 
  step_zv(all_predictors()) %>% 
  #date-time signature
  step_timeseries_signature(date) %>% 
  step_rm(matches("(.xts$)|(.iso$)|(hour)|(minute)|(second)|(am.pm)")) %>% 
  step_normalize(matches("(index.num)|(year)|(yday)")) %>% 
  
  # new binary column for each character column (TRUE is for all columns as binary)
  step_dummy(all_nominal_predictors(), one_hot = TRUE)
   


#non-sequential recipie
recipe_spec_ml<-recipe(weekly_sales ~ ., training(splits)) %>% 
  #clean up factor leveling
  step_mutate_at(id, fn = droplevels) %>% 
  step_normalize(temperature:unemployment) %>% 
  step_zv(all_predictors()) %>% 
  #date-time signature
  step_timeseries_signature(date) %>% 
  step_rm(matches("(.xts$)|(.iso$)|(hour)|(minute)|(second)|(am.pm)")) %>% 
  step_normalize(matches("(index.num)|(year)|(yday)")) %>% 
  
  #remove date for ml
  step_rm(date) %>% 
  
  
  # new binary column for each character column (TRUE is for all columns as binary)
  step_dummy(all_nominal_predictors(), one_hot = TRUE)
  


recipe_spec_ml %>% 
  prep() %>% 
  juice() %>% 
  glimpse()

```

#5.0 Models
```{r}
library(doParallel)
parallel_stop()

detectCores()
doParallel::registerDoParallel(cores = 7)
```


```{r}
## Naive
wflw_fit_naive <- workflow() %>%
  add_model(
    naive_reg(
      mode = "regression", 
      id = NULL, 
      seasonal_period = 366) %>% 
      set_engine("snaive")) %>%
  add_recipe(recipe_spec) 



# XGBOOST ----
# - Strengths: Best for seasonality & complex patterns
# - Weaknesses: 
#   - Cannot predict beyond the maximum/minimum target (e.g. increasing trend)
# - Solution: Model trend separately (if needed). 
#   - Can combine with ARIMA, Linear Regression, Mars, or Prophet
#   - prophet_boost & arima_boost: Do this
# XGBoost Models
wflw_fit_xgb_1 <- workflow() %>%
  add_model(boost_tree(
    mode = "regression",
    mtry = 25, 
    trees = 1000, 
    min_n = 2, 
    tree_depth = 12, 
    learn_rate = 0.3, 
    loss_reduction = 0
  ) %>%
    set_engine("xgboost")) %>%
  add_recipe(recipe_spec_ml) %>%
  fit(training(splits))

wflw_fit_xgb_2 <- workflow() %>%
  add_model(boost_tree("regression", learn_rate = 0.50) %>% set_engine("xgboost")) %>%
  add_recipe(recipe_spec_ml) %>%
  fit(training(splits))


# RANDOM FOREST ----
# - Strengths: Can model seasonality very well
# - Weaknesses: 
#   - Cannot predict beyond the maximum/minimum target (e.g. increasing trend)
# - Solution: Model trend separately (if needed). 
#   - Can combine with ARIMA, Linear Regression, Mars, or Prophet
# Random Forest
set.seed(123)
wflw_fit_rf <- workflow() %>%
  add_model(rand_forest(
    mode  = "regression", 
    mtry  = 25, 
    trees = 1000, 
    min_n = 25
  ) %>%
    set_engine("ranger")) %>%
  add_recipe(recipe_spec_ml) %>%
  fit(training(splits))


# Prophet
wflw_fit_prophet <- workflow() %>%
  add_model(spec = prophet_reg() %>% set_engine("prophet")) %>%
  add_recipe(recipe_spec) %>%
  fit(training(splits))


# Prophet Boost
wflw_fit_prophet_boost <- workflow() %>%
  add_model(
    spec = prophet_boost(
      # turn off all seasonality, as xgboost will pick this up
      seasonality_daily  = FALSE, 
      seasonality_weekly = FALSE, 
      seasonality_yearly = FALSE,
      
      # xgboost
      mtry = 25, 
      trees = 1000, 
      min_n = 1, 
      tree_depth = 6,
      learn_rate = 0.5, 
      loss_reduction = 0
    ) %>% 
      set_engine("prophet_xgboost")
    ) %>%
  add_recipe(recipe_spec) %>%
  fit(training(splits))


# THIEF
wflw_fit_thief <- workflow() %>%
  add_model(temporal_hierarchy() %>% set_engine("thief")) %>%
  add_recipe(recipe_spec) %>%
  fit(training(splits))


# ELASTIC NET REGRESSION ----
# - Strengths: Very good for trend
# - Weaknesses: Not as good for complex patterns (i.e. seasonality)
# GLMNet
wflw_fit_glmnet <- workflow() %>%
  add_model(
    linear_reg(
      mode = "regression", 
      penalty = 0.01, 
      mixture = 0) %>% 
    set_engine("glmnet")) %>%
  add_recipe(recipe_spec_ml) %>%
  fit(training(splits))


# MARS ----
# Multiple Adaptive Regression Splines
# - Strengths: Best algorithm for modeling trend
# - Weaknesses: 
#   - Not good for complex patterns (i.e. seasonality)
#   - Don't combine with splines! MARS makes splines.
# - Key Concept: Can combine with xgboost (better seasonality detection)
#   - prophet_reg: uses a technique similar to mars for modeling trend component
#   - prophet_boost: Uses prophet for trend, xgboost for features
# MARS
wflw_fit_mars <- workflow() %>%
  add_model(mars(
    mode = "regression", 
    num_terms = 10
  ) %>% 
    set_engine("earth", endspan = 200)) %>%
  add_recipe(recipe_spec) %>%
  fit(training(splits))


# SVM POLY ----
# Strengths: Well-rounded algorithm
# Weaknesses: Needs tuned or can overfit
# SVM Poly
wflw_fit_svm_poly <- workflow() %>%
  add_model(svm_poly(
    mode = "regression", 
    cost = 10, 
    degree = 1,
    scale_factor = 1,
    margin = 0.1
  ) %>%
    set_engine("kernlab")) %>%
  add_recipe(recipe_spec) %>%
  fit(training(splits))


# SVM RADIAL BASIS ----
# Strengths: Well-rounded algorithm
# Weaknesses: Needs tuned or can overfit
# SVM Radial
wflw_fit_svm_rdl <- workflow() %>%
  add_model(svm_rbf(
    mode = "regression",
    cost = 1, 
    rbf_sigma = 0.01,
    margin = 0.1
  ) %>%
    set_engine("kernlab")) %>%
  add_recipe(recipe_spec) %>%
  fit(training(splits))


# K-NEAREST NEIGHBORS ----
# - Strengths: Uses neighboring points to estimate 
# - Weaknesses: Cannot predict beyond the maximum/minimum target (e.g. increasing trend)
# - Solution: Model trend separately (if needed). 
#   - Can combine with ARIMA, Linear Regression, Mars, or Prophet
# Show issue when trend extends beyond maximum value ----
# K-Nearest Neighbors
set.seed(123)
wflw_fit_knn <- workflow() %>%
  add_model(nearest_neighbor(
    mode = "regression",
    neighbors = 50, 
    dist_power = 10, 
    weight_func = "optimal"
  ) %>%
    set_engine("kknn")) %>%
  add_recipe(recipe_spec_ml) %>%
  fit(training(splits))


# CUBIST ----
# - Like XGBoost, but the terminal (final) nodes are fit using linear regression
# - Does better than tree-based algorithms when time series has trend
# - Can predict beyond maximum
# Cubist
set.seed(123)
wflw_fit_cubist <- workflow() %>%
  add_model(cubist_rules(
    committees = 50, 
    neighbors = 7, 
    max_rules = 100
  ) %>%
    set_engine("Cubist")) %>%
  add_recipe(recipe_spec_ml) %>%
  fit(training(splits))


# NEURAL NET ----
# - Single Layer Multi-layer Perceptron Network
# - Simple network - Like linear regression
# - Can improve learning by adding more hidden units, epochs, etc
# Neural Net
set.seed(123)
wflw_fit_nnet <- workflow() %>%
  add_model(mlp(
    mode = "regression",
    hidden_units = 10,
    penalty = 1, 
    epochs = 100
  ) %>%
    set_engine("nnet")) %>%
  add_recipe(recipe_spec_ml) %>%
  fit(training(splits))


# NNETAR ----
# - NNET with Lagged Features (AR)
# - Is a sequential model (comes from the forecast package)
# - Must include date feature
# NNETAR
set.seed(123)
wflw_fit_nnetar <- workflow() %>%
  add_model(nnetar_reg(
    non_seasonal_ar = 2,
    seasonal_ar     = 1, 
    hidden_units    = 10,
    penalty         = 10,
    num_networks    = 10,
    epochs          = 50
  ) %>%
    set_engine("nnetar")) %>%
  add_recipe(recipe_spec) %>%
  fit(training(splits) %>% drop_na())




## ARIMA BOOST
wflw_fit_arima_boost <- workflow() %>%
  add_model(arima_boost(
    seasonal_period = 7,
    
    # xgboost
    mtry = 50, 
    trees = 1000,
    min_n = 7, 
    tree_depth = 12, 
    learn_rate = 0.7, 
    loss_reduction = 0
    )  %>%
  set_engine("arima_xgboost")) %>%
  add_recipe(recipe_spec) %>%
  fit(training(splits))
```


## Accuracy Check
```{r}
# * ACCURACY CHECK ----
submodels_1_tbl <- modeltime_table(
  wflw_fit_xgb_1,
  wflw_fit_xgb_2,
  wflw_fit_rf,
  wflw_fit_prophet,
  wflw_fit_prophet_boost,
  wflw_fit_thief,
  wflw_fit_glmnet,
  wflw_fit_mars,
  wflw_fit_svm_rdl,
  wflw_fit_knn,
  wflw_fit_cubist,
  wflw_fit_nnet,
  wflw_fit_nnetar
) %>%
  update_model_description(1, "xgb 1") %>%
  update_model_description(2, "xgb 2") %>%
  update_model_description(3, "rf") %>%
  update_model_description(4, "prophet") %>%
  update_model_description(5, "prophet boost") %>%
  update_model_description(6, "thief") %>%
  update_model_description(7, "glmnet") %>%
  update_model_description(8, "mars") %>%
  update_model_description(9, "svm rdl") %>%
  update_model_description(10, "knn") %>%
  update_model_description(11, "cubist") %>%
  update_model_description(12, "nnet") %>%
  update_model_description(13, "nnetar")
# calibrate on testing data -- LONG RUNNING SCRIPT!
submodels_calibrate <- submodels_1_tbl %>%
  modeltime_calibrate(new_data = testing(splits), id = "id")
# GLOBAL accuracy
submodels_calibrate %>%
  modeltime_accuracy(acc_by_id = FALSE) %>%
  arrange(rmse)
# ACCURACY BY ID
submodels_calibrate %>%
    modeltime_accuracy(acc_by_id = TRUE) %>%
    table_modeltime_accuracy()
```


```{r}
# 5.1 MODELS w/ TUNE
## Resamples
# K-Fold: Non-Sequential Models
set.seed(123)
resamples_kfold <- training(splits) %>% vfold_cv(v = 5)
# visualize the 10 folds (90/10 testing)
resamples_kfold %>%
    tk_time_series_cv_plan() %>%
    plot_time_series_cv_plan(date, weekly_sales, .facet_ncol = 2)
# TS Cross Validation: Sequential Models
resamples_tscv <- time_series_cv(
  data        = training(splits) %>% drop_na(),
  cumulative  = TRUE,
  assess      = 20,
  skip        = 7,
  slice_limit = 5
)
resamples_tscv %>%
  tk_time_series_cv_plan() %>%
  plot_time_series_cv_plan(.date_var = date, .value = weekly_sales)
```


```{r}
model_spec_xgboost_tune <- boost_tree(
    mode            = "regression",
    mtry            = tune(),
    trees           = 1000,
    min_n           = tune(),
    tree_depth      = tune(),
    learn_rate      = tune(),
    loss_reduction  = tune()
) %>%
    set_engine("xgboost")
wflw_spec_xgboost_tune <- workflow() %>%
    add_model(model_spec_xgboost_tune) %>%
    add_recipe(recipe_spec_ml)
# Tuning
set.seed(123)
tune_results_xgboost <- wflw_spec_xgboost_tune %>%
    tune_grid(
        resamples  = resamples_kfold,
        param_info = extract_parameter_set_dials(wflw_spec_xgboost_tune) %>%
            update(
                learn_rate = learn_rate(range = c(0.001, 0.400), trans = NULL)
            ),
        grid = 10,
        control = control_grid(verbose = TRUE, allow_par = TRUE)
    )
# ** Results
tune_results_xgboost %>% show_best("rmse", n = Inf)
# ** Finalize
set.seed(123)
wflw_fit_xgboost_tuned <- wflw_spec_xgboost_tune %>%
  finalize_workflow(select_best(tune_results_xgboost, "rmse")) %>%
  fit(training(splits))

```

## RF Tune
```{r, include=FALSE}
model_spec_rf_tune <- rand_forest(
    mode    = "regression",
    mtry    = tune(),
    trees   = tune(),
    min_n   = tune()
) %>%
    set_engine("ranger")
wflw_spec_rf_tune <- workflow() %>%
    add_model(model_spec_rf_tune) %>%
    add_recipe(recipe_spec_ml)
# ** Tuning
set.seed(123)
tune_results_rf <- wflw_spec_rf_tune %>%
    tune_grid(
        resamples = resamples_kfold,
        grid      = 5,
        control   = control_grid(verbose = TRUE, allow_par = TRUE)
    )
# ** Results
best_rf_tuned <- tune_results_rf %>% show_best("rmse", n = Inf)
# ** Finalize
set.seed(123)
wflw_fit_rf_tuned <- wflw_spec_rf_tune %>%
  finalize_workflow(select_best(tune_results_rf, "rmse")) %>%
  fit(training(splits))
```




## KNN Tune
```{r, include=FALSE}
model_spec_knn_tune <- nearest_neighbor(
    mode        = "regression",
    neighbors   = tune(),
    dist_power  = tune(),
    weight_func = "optimal"
) %>%
    set_engine("kknn")
wflw_spec_knn_tune <- workflow() %>%
    add_model(model_spec_knn_tune) %>%
    add_recipe(recipe_spec_ml)
# ** Tuning
set.seed(123)
tune_results_knn <- wflw_spec_knn_tune %>%
    tune_grid(
        resamples = resamples_kfold,
        grid      = 5,
        control   = control_grid(verbose = TRUE, allow_par = TRUE)
    )
# ** Results
best_knn_tuned <- tune_results_knn %>% show_best("rmse", n = Inf)
# ** Finalize
set.seed(123)
wflw_fit_knn_tuned <- wflw_spec_knn_tune %>%
  finalize_workflow(select_best(tune_results_knn, "rmse")) %>%
  fit(training(splits))
```




# 6.0 EVALUATE FORECAST
## Combine tuned & submodels
```{r}
submodels_2_tbl <- modeltime_table(
  wflw_fit_xgboost_tuned,
  wflw_fit_rf_tuned,
  wflw_fit_knn_tuned
) %>%
  update_model_description(1, "xgboost tuned") %>%
  update_model_description(2, "rf tuned") %>%
  update_model_description(3, "knn tuned") %>%
  combine_modeltime_tables(submodels_1_tbl)
# calibrate
calibration_tbl <- submodels_2_tbl %>%
  modeltime_calibrate(
    new_data = testing(splits),
    id = "id")
calibration_tbl %>% 
  modeltime_accuracy(acc_by_id = FALSE) %>% 
  arrange(rmse)

# Accuracy
calibration_tbl %>%
  modeltime_accuracy(acc_by_id = T) %>%
  table_modeltime_accuracy()
# forecast on daily_prepared_tbl
forecast_test_tbl <- calibration_tbl %>%
  modeltime_forecast(
    new_data    = testing(splits),
    actual_data = weekly_prepared_tbl,
    conf_by_id  = T,
    keep_data   = T) %>%
  mutate(across(
    .cols = c(.value:.conf_hi, weekly_sales),
    .fns  = expm1
  ))
# visualize
forecast_test_tbl %>%
  group_by(id) %>%
  plot_modeltime_forecast(
    .conf_interval_show = F,
    .trelliscope = T,
    .facet_ncol  = 3,
    .title = "All Test Forecasts")
```




#Select Best
```{r}
#select the best with RMSE

calibration_best_tbl<- calibration_tbl %>% 
  modeltime_accuracy(acc_by_id = TRUE) %>% 
  group_by(id) %>% 
  slice_min(rmse) %>% 
  #in case of exact rmse duplicates
  slice_min(.model_id) %>% 
  ungroup()
  
#Check Accuracy

calibration_best_tbl %>% 
  table_modeltime_accuracy()

```


```{r}
data_forecasted <- calibration_baseline_tbl %>%
    modeltime_forecast(
        new_data    = testing(splits),
        actual_data = weekly_prepared_tbl,
        conf_by_id  = TRUE,
        keep_data   = TRUE
    )

#### FUNCTION: keep up to specific column name #####
keep_up_to <- function(df, colname){
  col_i <- which(colnames(data_forecasted) == colname)
  df[(1:col_i)]
}
# example...
data_forecasted %>%
  keep_up_to("2012-10-26")
# create test forecast for best models
final_baseline_models <- data_forecasted %>%
  filter(.key == "actual") %>%
  bind_rows(
    data_forecasted %>%
      filter(.key == "prediction") %>%
      inner_join(calibration_best_tbl, by = c("id", ".model_id")) %>%
      keep_up_to(rev(names(data_forecasted))[1]) %>%
      rename(.model_desc = .model_desc.x)
  )
# visualize
final_baseline_models %>%
  group_by(id) %>%
  plot_modeltime_forecast(
    .conf_interval_show = F,
    .trelliscope = T,
    .facet_ncol  = 3,
    .title = "Best Test Forecasts")
```
#Create full tbl in model time format
```{r}
full_data_prepared_tbl %>% 
  group_by(id) %>% 
  filter(date < min(weekly_prepared_tbl))

```


#7.0 Forecast The Future
```{r}
#Refit on to-date dataset
calibration_baseline_tbl %>% 
  model_time



model
```






